1. use case:
    a. backup and storage
    b. disaster recovery
    c. archive
    d. hybrid cloud storage(with on-premise storage)
    e. application hosting
    f. media hosting
    g. static website
    h. data lake / big data analytics
    ...

2. stores objects(files) in buckets(directory)

3. Bucket must have GLOBALLY UNIQUE name

4. Buckets are defined at region level (specify a region before creating one.)

5. S3 is a global service, but buckets are defined at region level.

6. bucket name: can consist only of lowercase letters, numbers, dots (.), and hyphens (-).
    a. no uppercase, 
    b. no underscore
    c. 3-63 char long.
    d. not an ip
    e. must start with lowercase letter or number
    f. must NOT start with prefix xn--
    g. must NOT end with suffix -3alias

7.Objects(files) have a key. The key is full path of the corresponding object(file).
    example: my_folder/my_file.txt, my_folder/my_other_folder/my_file.txt...
    Key = prefix + object name

8.no actual folders in a bucket, but long object keys with slashes: my_folder/my_other_folder/my_file.txt

9. Object values are content of the body. Max object is 5TB.

10. if uploading size > 5GB, "multi-part" upload is a must.

11. object metadata: Each object in an S3 bucket can have associated metadata, which are key-value pairs that provide additional information about the object. (example: Content-Type, Last-Modified)

12. object tags: 
    a. Tags are simple key-value pairs that help manage and control access to S3 objects. 
    b. Each tag consists of a key and a value, both of which are defined by you. For example, a tag could be {"Key": "Environment", "Value": "Production"}. You can assign up to 50 tags per S3 object.

13. version id: Each version of an object is given a unique version ID. When you enable versioning on a bucket, S3 generates a version ID for each object uploaded.

14. Resource-based policy:
    a. Bucket policies: bucket wide rules you can assign from the S3 console - allow user from another account to access(cross account)  (most common)
    b. Object Access Control List(ACL) - finer grain(on object level) (can be disabled)
    c. Bucket Access Control List(ACL) - less common(can be disabled)

15. an IAM principal can access an S3 object if:
    The user IAM permissions explicitly ALLOW it OR the resource policy explicitly ALLOWS it, AND there is no explicit DENY.

16. Encryption: encrypt objects in S3 using encryption keys.

17. Bucket Policies(JSON based policies):
    a. Resouces: what buckets and objects this policy applies on. (example: "arn:aws:s3:::examplebucket/*")
    b. Effect: Allow / Deny
    c. Actions: Set of API to Allow or Deny. (example: "s3: GetObject")
    d. Principal: The account or user to apply the policy to (example(allow anyone): "*")

18. Use S3 bucket for policy to: 
    a. Grant public access to the bucket
    b. force objects to be encrypted at upload
    c. Grant access to another account(cross account)

19. example policies in JSON format:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::examplebucket/*"
        }
    ]
}

Each element in the Statement array represents an individual policy statement.

20. To access a S3 bucket:
     For user: you need at least one explicit Allow from either IAM USER policy or Bucket policies, without explicit Deny anythere.
     For services: you need at least one explicit Allow from either IAM ROLE or Bucket policies, without explicit Deny anythere.
     For cross-account access: you must use bucket policy to explictly allow it.

21. Bucket settings for block public access. (Set when we create a bucket).
    a. These settings were created to prevent data leaks.
    b. EXTRA layer of security. (hence you need the Allow here AND in the relevant policy to ACTUALLY allow access)
    c. if you know your bucket should never be public, leave these on.
    d. if you know NONE of your buckets should ever be public, you can set it at the account level. (once its set, you cannot make any exception to any particular buckets, as this overrides all)

22. S3 can host static website. (typical url looks like this: http://demo-bucket.s3-website-us-west2.amazonaws.com). Users will access our S3 bucket. 403 forbidden => bucket policy does not allow public read, change it!

23. Versioning. Enabled at bucket level. once enabled, we can upload different files to the same key, version 1, version 2....
    a. protect against unintended deletes(ability to restore)
    b.easy to roll back to previous version.
    Note: any file that is not versioned prior to enabling versioning will have version "null" and suspending versioning does NOT delete prev versions.

24. with versioning enabled:

    a.delete with show version toggled on, it actually delete the file(destructive), so deleting version 4 will bring you to version 3 of the file in the end.

    b.delete with show version toggled off, it actually add a new version on top of the current one(like revert in git), with the type being "Delete marker". The current file(version), can be restored.(by deleting the delete marker with show version toggle on)

25. Replication(CRR & SRR)
    a. CRR: Cross Region Replication (compliance, lower latency access, replication across accounts)
    b. SRR: Same Region Replication (log aggregation, live replication between production and test accounts, so we can have data to test on.)

26. For example: setting up async replication between these bucket: bucket1(eu-west-1) and bucket2(us-east-2):
    a. must enable Versioning in source and destination buckets.
    b. bucket can be in different AWS account.
    c. copying is async
    d. must give proper IAM permissions to S3. (IAM role for source so it can replicate in destination)

27.After you enable replication, ONLY new objects are replicated.

28. Optionally, you can replicate existing objects using S3 Batch Replication.
    a. replicate existing objects and objects that failed replication.

29. Delete:
    a. can replicate delete markers from source to target(optional setting)
    b. delete with version id(destructive) are NOT replicated(to avoid malicious delete)
    c. by default, delete markers are not replicated, but you can update the setting in "management" => "replication rules"

30. No chain:
    if bucket 1 has replication into bucket 2, which has replication into bucket 3, then objects created in bucket 1 are NOT replicated to bucket 3.
    
    (When bucket 2 receives this object, it does not automatically replicate it to bucket 3, even if there's a replication configuration between bucket 2 and bucket 3. This is because the replication process does not pass the "new object" event from bucket 1 through to bucket 3.)

    In short, bucket 3 will only receive objects that are ORIGINALLY created in bucket 2, not the ones that already a replication themselves.

31. Storage Classes:
    a. Standard - General purpose
    b. Standard - Infrenquent Access(IA)
    c. One Zone-Infrequent Access
    d. Glacier Instant Retrieval
    e. Glacier Flexibal Retrieval
    f. Glacier Deep Archive
    g. Intelligent Tiering

    Objects stored in the same bucket can have different storage classes.

    Can move between classes manually(can edit after) or using S3 Lifecycle configurations

32. Durability:
    a. Durability: High Durability(99.99999999%, 11 9's) of objects across multiple AZ.
    b. if you store 10,000,000 objects with s3, you can on average expect to incur a loss of a single object once every 10000 years.
    c. same for all storage classes.

33 Availability:
    a. measures how readily available a service is
    b. varies depending on storage class
    c. Example: S3 standard has 99.99% availability = not available 53 minutes a year.

33. Standard Class:
    1.General purpose:
        a. 99.99% availability
        b. used for FREQUENT accessed data.
        c. low latency and high throughput
        d. can sustain 2 concurrent facility failures.

        use cases: Big Data analytics, mobile/gaming applications, content distribution...

    2.Infrequent Access(S3 Standard-IA):
        a. for data that is less frequently accessed, but requires rapid access when needed.
        b. lower cost than S3 standard.
        c. but there is a cost for retrieval
        d. 99.9% availability.

        use cases: disaster recovery, backups

    3.One Zone-Infrequent Access(S3 One Zone-IA):
        a. High durability(99.999999999%) in a single AZ, and data is LOST when AZ is destroyed.
        b. 99.5% availability.
        use case: storing secondary backup copies of on premise data, or data you can recreate

34. Glacier Class:
    a. low-cost object storage meant for archiving / backup
    b. Pricing: price for storage + object retrieval cost

    1. Instant Retrieval:
        a. millisecond retrieval, great for data accessed once a quarter.(Itâ€™s ideal for archive data that might need to be accessed more spontaneously, such as for quarterly financial reports or rare audits.)

        b. min storage duration of 90 days. Data stored for less than this duration incurs charges as if it were stored for the full 90 days, making it less suitable for short-term storage.

    2. Flexibal Retrieval(formerly S3 glacier):
        a. retrieval speed:
            Expedited(1 to 5 minutes)
            Standard(3 to 5 hours)
            Bulk(5 to 12 hours) - free
        
        b.min storage duration: 90 days

    3. Deep Archive(for long term storage):
        a. standard(12 hours), bulk(48hours)
        b. min storage duration of 180 days.

35. Intelligent-Tiering
    a. Small monthly monitoring and auto-tiering fee.
    b. Moves object automatically between access tiers based on usage.
    c. no retrieval charges.

    1. Frequent access tier(automatic): default tier
    2. Infrequent access tier(automatic): objects not accessed for 30 days.
    3. Archive Instant Access tier(automatic): objects not accessed for 90 days.
    4. Archive Access tier(optional): configurable from 90 days to 700+ days.
    5. Deep Archive Access Tier(optional): configurable from 180 days to 700+ days.
    

36. Lifecycle Rules: You can transition objects between storage classes, which can be automated with Lifecycle rules.
    1. Transition Actions: configure objects to transition to another storage class.
        example: move objects to standard IA class 60 days after creation
    
    2. Expiration Actions: configure objects to expire(delete) after some time. (perm delete, not marker!)
        example: access log files can be set to delete after a 365 days.
        can be used to delete old versions(noncurrent versions) of files(if versioning is enabled)
        can be used to delete incomplete multi-part uploads(like if it not done within a week, delete the finished part)
    3. Rules can be created for:
        1. a certain prefix(example: s3://mybucket/mp3/*)
        2. certain objects Tags(example: Department: Finance)

    4. Scenarios: check course 140, starting at 1:50.

37. S3 Analytics: Storage class analysis.
    1. help you decide when to transition objects to the right storage class
    2. Recommendations for Standard and Standard IA:
        a. does NOT work for ONE-ZONE IA or Glacier
    3. S3 analytics will run on top of a S3 bucket and create a .csv report with the recommendations.
    4. the .csv report is updated daily.
    5. takes 24 to 48 hours to start seeing data analysis from it.
    6. it is a good first step to put together a lifecyle rules.(or improve it)

38. S3 Requester Pays.
    1. In general, bucket owners pay for all S3 related costs associated with their bucket. (storage + data transfer).
    2. With Requester Pays buckets, the request instead of the owner pays the cost of the request and the data download from the bucket. The owner still pays for the storage cost.
    3. The requester MUST be authenticated in AWS(cannot be anonymous), so AWS knows (how to) bill that requester. (In the "Requester Pays" model, the requester must have an active AWS account with valid billing information. This is because AWS charges the requester's account for the data transfer and request costs associated with accessing the S3 bucket. Since anonymous access is not allowed, each request must be authenticated using AWS credentials, ensuring that AWS can track and bill the correct account for the usage.)

39. S3 Event Notifications
    1. Event: S3:ObjectCreated, S3:ObjectRemoved...
    2. Object name filtering possible (*.jpg)
       use case: automatically generate thumbnails of images uploaded to s3.
    3. Can create as many S3 events as desired.
    4. Destinations: SNS, SQS, Lamdda Function.
    5. typically delivers in seconds but CAN take a minute or longer
    6. we use resource policies INSTEAD OF IAM Roles: SNS Resource(Access) Policy, SQS Resource(Access) Policy, Lambda Resource Policy.  (course 143, at 1:47)

40. Amazon EventBridge
    1. instead of sending events directly to destinations above, you can first send them all to Amazon EventBridge.
    2. you can set rules and send these events to over 18 aws services from eventbridge.
    3. you get Advanced filtering options with JSON rules(metadata, object size, name...)
    4. you can send events to Multiple Destinations at a time. (ex: Step Functions, Kinesis Streams/Firehose)
    5. EventBridge Capabilities: Archive, Replay Events, Reliable delivery.

41. S3 Baseline Performance
    1. automatically scales to high request rates, latency 100-200ms.
    2. can achieve at least 3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests per second per PREFIX in a bucket.
    3. no limits to the number of prefixes in a bucket.
    4. example(object path => prefix):
        bucket/folder1/sub1/file => /folder1/sub1/
        bucket/folder1/sub2/file => /folder1/sub2/
        bucket/1/file => /1/
        bucket/2/file => /2/
    If you spread reads across all four prefixes evenly, you can achieve 22000 requests per second for GET and HEAD.

42. Optimization:
    1. Multi-part upload:
        a. recommended for files > 100 MB, must use for files > 5GB
        b. can help parallelize uploads(speed up transfers): divide in parts, upload each at the same time(parallel uploads), then assemble into one at destination.
    2. S3 Transfer Acceleration:
        a. increase transfer speed by transferring file to an nearing AWS edge location which will then forward the data to the S3 bucket in the target region, using AWS private network(much faster than internet)
        b. compatiable with multi-part upload.
    3. S3 Byte-Range Fetches:
        a. Parallelieze GETs by requesting specific byte ranges.
        b. Better resilience in case of failures. (you can retry a small part if it fails, instead of retrying the whole thing)
        c. use case: 
            1. can be used to speed up downloads.
            2. can be used to retrieve only partial data(for example the head of a file, first XX bytes)
 
 43. S3 Select & Glacier Select
    1. Retrieve less data using SQL by performing server-side filtering.
        a.Can filter by rows & columns(simple SQL statement)
        b.Less network transfer, less CPU cost on client-side.
    
 44. S3 Batch Operations
    1. Perform bulk operations on existing S3 objects with a single request, example:
        a. Modify object metadata and properties.
        b. Copy objects between S3 buckets.
        c. Encrypt un-encrypted objects.
        d. Modify ACLs, tags.
        e. Restore objects from S3 Glacier.
        f. Invoke Lambda function to perform custom action on each object.
    2. A JOB consists of a list of objects, the action to perform, and optional parameters.
    3. Batch Operations manages retries, tracks progress, send completion notifications, generate reports...
    4. To generate a list of objects to be passed to Batch Operations, we can use S3 Inventory to get object list and use S3 Select to filter our objects, then pass them to Batch Operations, then Batch Operation will return a list of Processed Objects once its done.

45. S3 Storage Lens
    1. Understand, analyze and optimize storage across entire AWS Organization.
    2. Discover anomalies, identify cost efficiencis and apply data protection best practice across entire AWS organization(30 day usage and activity metrics)
    3. Aggregate data for Organization, specific accounts, regions, buckets, or prefixes.
    4. Default dashboard or create your own dashboard.
    5. can be configure to export metrics daily to a S3 bucket(CSV, Parquet)
    6. Default Dashboard:
        a. Visualize summarized insights and trends for both free and advanced metrics.
        b. It shows Multi-Region and Multi-Account Data.
        c. Preconfigured by S3.
        d. can't be deleted.
    7. Metrics:
        a. Summary Metrics:
            1. General insights about your S3 storage.
            2. StorageBytes, ObjectCount..
            3. Use cases: identify the fastest-growing(or not used) buckets and prefixes.
        b. Cost-Optimization Metrics:
            1. Provide insights to manage and optimize your storage costs.
            2. NonCurrentVersionStorageBytes, IncompleteMutipartUploadStorageBytes...
            3. Use cases: identify buckets with incomplete multipart uploaded older than 7 days, identify which objects could be transitioned to lower-cost storage class.
        c. Data-Protection Metrics:
            1. Provide insights for data protection features.
            2. VersioningEnabledBucketCount, MFADeleteEnabledBucketCount, SSEKMSEnabledBucketCount, CrossRegionReplicationRuleCount...
            3. Use cases: identify buckets that aren't following data-protection best practices.
        d. Access-management Metris:
            1. Provide insights for S3 Object Ownership.
            2. ObjectOwnershipBucketOwnerEnforcedBucketCount...
            3. Use cases: identify which object ownership settings your buckets use
        e. Event Metrics:
            1. Provide insights for S3 Event Notifications
            2. EventNotificationEnabledBucketCount
            3. Use cases: Identify which buckets have S3 event notifications configured.
        f. Performance Metrics:
            1. Provide insights for S3 transfer accelaration.
            2. TransferAccelerationEnabledBucketCount.
            3. Use cases: identify which buckets have S3 transfer accelaration enabled.
        g. Activity Metrics:
            1. Provide insights about how your storage is requested. 
            2. AllRequests, GetRequests, PutRequests, ListRequests...
        h. Detailed Status Code Metris:
            1. Provides insights for HTTP status code.
            2. 200OKStatusCount, 403ForbiddenErrorCount, 404NotFoundErrorCount...
    8. Free vs Paid
        1. Free Metrics:
            a. Automatically available for all customers.
            b. Contains around 28 usage metrics.
            c. data available for queries for 14 days.
        2. Advanced Metrics and Recommendations:
            a. additional paid metrics and features.
            b. Advanced Metrics: Activity, Advanced Cost Optimization, Advanced Data Protection, Status Code...
            c. CloudWatch Publishing: Access metrics in CloudWatch without additional cost.
            d. Prefix Aggregation: Collect metrics at the prefix level.
            e. data available for queries for 15 months.